import dotenv from "dotenv";
import { getGenerativeModel } from "../utils/gemini.js";

dotenv.config();

// Danh sÃ¡ch model kháº£ dá»¥ng (cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y key báº¡n Ä‘Æ°á»£c cáº¥p)
const MODEL_CANDIDATES = [
  "gemini-2.0-flash",
  "gemini-1.5-flash",
  "gemini-1.5-pro",
];

/**
 * ðŸ§  HÃ m xá»­ lÃ½ chat AI (gá»i má»™t láº§n, khÃ´ng stream)
 */
export async function chat(req, res) {
  try {
    const { message, history } = req.body;
    if (!message) {
      return res.status(400).json({ error: "message is required" });
    }

    // Chuáº©n hÃ³a lá»‹ch sá»­ há»™i thoáº¡i
    const contents = [
      ...(history || []).map((m) => ({
        role: m.role,
        parts: [{ text: m.content }],
      })),
      { role: "user", parts: [{ text: message }] },
    ];

    // Thá»­ láº§n lÆ°á»£t cÃ¡c model kháº£ dá»¥ng
    let responseText = "";
    let modelUsed = "";

    for (const modelId of MODEL_CANDIDATES) {
      try {
        const model = getGenerativeModel(modelId);

        const result = await model.generateContent({
          contents,
          generationConfig: { temperature: 0.6, maxOutputTokens: 1024 },
        });

        responseText = result.response.text();
        modelUsed = modelId;
        break; // thÃ nh cÃ´ng => dá»«ng
      } catch (err) {
        console.warn(`[AI] Model ${modelId} failed:`, err.message);
        continue;
      }
    }

    if (!responseText) {
      throw new Error("All models failed to respond");
    }

    res.json({ reply: responseText, model: modelUsed });
  } catch (err) {
    console.error("[AI] /api/chat error:", err);
    res.status(500).json({ error: err.message });
  }
}

/**
 * âš¡ HÃ m xá»­ lÃ½ chat stream (náº¿u báº¡n muá»‘n realtime)
 */
export async function chatStream(req, res) {
  try {
    const { message, history } = req.body;
    if (!message) {
      return res.status(400).json({ error: "message is required" });
    }

    const contents = [
      ...(history || []).map((m) => ({
        role: m.role,
        parts: [{ text: m.content }],
      })),
      { role: "user", parts: [{ text: message }] },
    ];

    const modelId = "gemini-1.5-flash";
    const model = getGenerativeModel(modelId);

    const streamingResult = await model.generateContentStream({
      contents,
      generationConfig: { temperature: 0.6, maxOutputTokens: 1024 },
    });

    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");

    for await (const chunk of streamingResult.stream) {
      const chunkText = chunk.text();
      if (chunkText) {
        res.write(`data: ${JSON.stringify({ text: chunkText })}\n\n`);
      }
    }

    res.write("data: [DONE]\n\n");
    res.end();
  } catch (err) {
    console.error("[AI] /api/chatStream error:", err);
    res.status(500).json({ error: err.message });
  }
}
